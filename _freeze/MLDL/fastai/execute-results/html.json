{
  "hash": "c42aa99ec7885d71f2bbca7b01342365",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: A quick application of DL for image Classification\nformat:\n  html:\n    code-fold: false\nexecute:\n  freeze: true\n---\n\n## Data Prep / loading\nBefore creating an Image classifier it is necessary to gather a dataset of images representing the different categories you wish to classify. \nFor my example I used a browser extension to download images as I browsed real-estate websites (zillow, immowelt, suumo, etc.). \nNext I used an ipynb I made to sort the dataset <https://github.com/pickpj/img-labeler-nb/blob/main/sep.ipynb> .\n\nNext we: import libraries, check folder for images that can't be opened, and load the images into a DataBlock. The DataBlock handles train/test split (splitter), and transformations (item_tfms). However, we will still need to have a separate test dataset, which can be done at a later time.\nThe command `du --inodes -d 2` counts the number of files (490 images), from this we can see the data is balanced between the two categories (57.8:42.2%). \n\n::: {#332b10d5 .cell execution_count=1}\n``` {.python .cell-code}\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\n\nfp = Path(\"./Data\")\n\nfailed = verify_images(get_image_files(fp))\nfailed.map(fp.unlink)\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    get_y=parent_label,\n    splitter=RandomSplitter(valid_pct=0.25),\n    item_tfms=[Resize(192, method=\"squish\")]\n).dataloaders(fp)\n\nos.system(\"(cd Data && du --inodes -d 2)\")\ndblock.show_batch(max_n=9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n283\t./Interior\n207\t./Exterior\n491\t.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](fastai_files/figure-html/cell-2-output-2.png){width=688 height=704}\n:::\n:::\n\n\n## Training / Fine-tuning\nWe then use an existing image model/pre-trained weights (arch) of weights and fine tune it to our dataset. Resnet18 is a small model the can be trained quickly with great performance. There are better models like swin_t, but the gains are marginal and training is longer. More SotA models can be found on the pytorch site <https://pytorch.org/vision/stable/models.html#classification> .\n\n::: {#5450f833 .cell execution_count=2}\n``` {.python .cell-code}\nvismodel = vision_learner(dblock, resnet18, metrics=error_rate)\nvismodel.fine_tune(3)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>error_rate</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.893850</td>\n      <td>0.085667</td>\n      <td>0.032787</td>\n      <td>00:26</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>error_rate</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.096202</td>\n      <td>0.013392</td>\n      <td>0.008197</td>\n      <td>00:36</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.058436</td>\n      <td>0.004852</td>\n      <td>0.000000</td>\n      <td>00:35</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.043074</td>\n      <td>0.006278</td>\n      <td>0.000000</td>\n      <td>00:38</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Results\nFrom there the model can be exported/saved. To use the saved model there is the `load_learner` and `.predict()` functions. \n\n::: {#2dfd9f38 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nvismodel.export(\"model.pkl\")\nvm = load_learner(\"model.pkl\")\n```\n:::\n\n\n\n\nThe predict function outputs the result and a tensor containing the probabilities of each category. \n\n::: {#4babc42f .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nprint(vm.predict(\"interior.webp\"))\nprint(vm.predict(\"difficult-interior.webp\"))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('Interior', tensor(1), tensor([5.3082e-05, 9.9995e-01]))\n('Interior', tensor(1), tensor([0.0440, 0.9560]))\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n:::\n\n\n::: {layout-ncol=2}\n![interior](extra-fastai/interior.webp)\n\n![difficult-interior](extra-fastai/difficult-interior.webp)\n:::\nAs can be seen from the output the model is able to identify both images of interior with high confidence. The \"difficult-interior\" image still performed well with ~95% confidence.\n\n",
    "supporting": [
      "fastai_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}