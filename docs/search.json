[
  {
    "objectID": "MLDL/tabtools.html",
    "href": "MLDL/tabtools.html",
    "title": "A collection of some code I find useful",
    "section": "",
    "text": "I will find a dataset to show each example, but for now it’s just the code.\nSeaborn is imported as sns.\nThe pandas dataframe is stored as d\n\npplot = sns.pairplot(d.sample(400)[cols], plot_kws={\"marker\":\"+\", \"linewidth\":1})\npplot.map_lower(sns.kdeplot, levels=4, color=\".2\")\n\nThe pairplot function is useful to quickly create scatterplots for all of the variables in a table (in the list variable “cols”). With the scatterplots it is easy to identify trends in the data. Additionally with the kdeplot (kernel density estimation) highlights where datapoints are clustered in each scatterplot. Sampling is used to prevent datapoints from overcrowding in large datasets.\n\nsns.kdeplot(data=d, x=\"xcolname\", \\\n            y=\"ycolname\", fill=True, hue=\"left\")\n\nAfter noticing something in the pairplot kdeplot would be used to get a closer look.\n\ncol = [\"a\",\"b\"]\nfor i in col:\n    up = d[i].quantile(.75)\n    low = d[i].quantile(.25)\n    iqr = up-low\n    d_range = iqr*1.5\n    d = d[(d[i]&lt;(d_range+up))&(d[i]&gt;(low-d_range))]\n\nJust a loop to get rid of outliers\n\nd = pd.get_dummies(d, columns=[\"categoricalcol\", \"categoricalcol\"], drop_first=False)\nX = d.drop(\"ycol\", axis=1)\ny = d.ycol\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\nxgb = XGBClassifier(objective=\"binary:logistic\")\n\ncv_params={'max_depth': [8],\n           'learning_rate':[0.01,0.1],\n           'min_child_weight':[4],\n           'n_estimators': [300]}\nscoring = {\"f1\",\"recall\",\"accuracy\",\"precision\"}\nxgbm = GridSearchCV(xgb, cv_params, scoring=scoring, cv=3, refit=\"f1\")\n\nxgbm.fit(X_train, y_train)\nplot_importance(xgbm.best_estimator_)\n\nA boilerplate of sorts for xgboost. As for what parameters to use ¯\\_(ツ)_/¯ (More scoring methods: https://scikit-learn.org/stable/modules/model_evaluation.html )",
    "crumbs": [
      "ML/DL",
      "Tabular",
      "Tools"
    ]
  },
  {
    "objectID": "MLDL/fastai.html",
    "href": "MLDL/fastai.html",
    "title": "A quick application of DL for image Classification",
    "section": "",
    "text": "Before creating an Image classifier it is necessary to gather a dataset of images representing the different categories you wish to classify. For my example I used a browser extension to download images as I browsed real-estate websites (zillow, immowelt, suumo, etc.). Next I used an ipynb I made to sort the dataset https://github.com/pickpj/img-labeler-nb/blob/main/sep.ipynb .\nNext we: import libraries, check folder for images that can’t be opened, and load the images into a DataBlock. The DataBlock handles train/test split (splitter), and transformations (item_tfms). However, we will still need to have a separate test dataset, which can be done at a later time. The command du --inodes -d 2 counts the number of files (490 images), from this we can see the data is balanced between the two categories (57.8:42.2%).\n\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\n\nfp = Path(\"./Data\")\n\nfailed = verify_images(get_image_files(fp))\nfailed.map(fp.unlink)\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    get_y=parent_label,\n    splitter=RandomSplitter(valid_pct=0.25),\n    item_tfms=[Resize(192, method=\"squish\")]\n).dataloaders(fp)\n\nos.system(\"(cd Data && du --inodes -d 2)\")\ndblock.show_batch(max_n=9)\n\n283 ./Interior\n207 ./Exterior\n491 .",
    "crumbs": [
      "ML/DL",
      "Image",
      "Image Classification"
    ]
  },
  {
    "objectID": "MLDL/fastai.html#data-prep-loading",
    "href": "MLDL/fastai.html#data-prep-loading",
    "title": "A quick application of DL for image Classification",
    "section": "",
    "text": "Before creating an Image classifier it is necessary to gather a dataset of images representing the different categories you wish to classify. For my example I used a browser extension to download images as I browsed real-estate websites (zillow, immowelt, suumo, etc.). Next I used an ipynb I made to sort the dataset https://github.com/pickpj/img-labeler-nb/blob/main/sep.ipynb .\nNext we: import libraries, check folder for images that can’t be opened, and load the images into a DataBlock. The DataBlock handles train/test split (splitter), and transformations (item_tfms). However, we will still need to have a separate test dataset, which can be done at a later time. The command du --inodes -d 2 counts the number of files (490 images), from this we can see the data is balanced between the two categories (57.8:42.2%).\n\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\n\nfp = Path(\"./Data\")\n\nfailed = verify_images(get_image_files(fp))\nfailed.map(fp.unlink)\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    get_y=parent_label,\n    splitter=RandomSplitter(valid_pct=0.25),\n    item_tfms=[Resize(192, method=\"squish\")]\n).dataloaders(fp)\n\nos.system(\"(cd Data && du --inodes -d 2)\")\ndblock.show_batch(max_n=9)\n\n283 ./Interior\n207 ./Exterior\n491 .",
    "crumbs": [
      "ML/DL",
      "Image",
      "Image Classification"
    ]
  },
  {
    "objectID": "MLDL/fastai.html#training-fine-tuning",
    "href": "MLDL/fastai.html#training-fine-tuning",
    "title": "A quick application of DL for image Classification",
    "section": "Training / Fine-tuning",
    "text": "Training / Fine-tuning\nWe then use an existing image model/pre-trained weights (arch) of weights and fine tune it to our dataset. Resnet18 is a small model the can be trained quickly with great performance. There are better models like swin_t, but the gains are marginal and training is longer. More SotA models can be found on the pytorch site https://pytorch.org/vision/stable/models.html#classification .\n\nvismodel = vision_learner(dblock, resnet18, metrics=error_rate)\nvismodel.fine_tune(3)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.926669\n0.226859\n0.065574\n00:30\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.072545\n0.030340\n0.008197\n00:37\n\n\n1\n0.060466\n0.024998\n0.008197\n00:38\n\n\n2\n0.040806\n0.028396\n0.008197\n00:38",
    "crumbs": [
      "ML/DL",
      "Image",
      "Image Classification"
    ]
  },
  {
    "objectID": "MLDL/fastai.html#results",
    "href": "MLDL/fastai.html#results",
    "title": "A quick application of DL for image Classification",
    "section": "Results",
    "text": "Results\nFrom there the model can be exported/saved. To use the saved model there is the load_learner and .predict() functions.\n\nvismodel.export(\"model.pkl\")\nvm = load_learner(\"model.pkl\")\n\nThe predict function outputs the result and a tensor containing the probabilities of each category.\n\nprint(vm.predict(\"interior.webp\"))\nprint(vm.predict(\"difficult-interior.webp\"))\n\n\n\n\n('Interior', tensor(1), tensor([5.3082e-05, 9.9995e-01]))\n('Interior', tensor(1), tensor([0.0440, 0.9560]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterior\n\n\n\n\n\n\n\ndifficult-interior\n\n\n\n\n\nAs can be seen from the output the model is able to identify both images of interior with high confidence. The “difficult-interior” image still performed well with ~95% confidence.",
    "crumbs": [
      "ML/DL",
      "Image",
      "Image Classification"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "A website to showcase some stuff ¯\\_(ツ)_/¯"
  },
  {
    "objectID": "evmorse.html",
    "href": "evmorse.html",
    "title": "Macros independent of the windowing system",
    "section": "",
    "text": "When it comes to macros and key bindings in Wayland, the situation is pretty bad. People may point to X11 applications being able to see keystrokes as being insecure, but when it comes to usability… I couldn’t care less. Additionally, Wayland’s lack of window managing tools like wmctrl and xdotool makes a transition to Wayland seem impossible. However, the first part is solvable! *kinda* . With the drawback of needing elevated permissions to: read device event files, create udev rules, and altering the hardware database. You too can have proper device specific macro/keybinding’s on any linux system.",
    "crumbs": [
      "Keyboard",
      "ev-morse"
    ]
  },
  {
    "objectID": "evmorse.html#ev-morse",
    "href": "evmorse.html#ev-morse",
    "title": "Macros independent of the windowing system",
    "section": "ev-morse",
    "text": "ev-morse\nEv-morse is a tool I made with the specific use case of binding macros to double key presses or long key presses. I use it with keyboards, headphones, and a dial. But it could be adapted to be used with any device that outputs to /dev/input (Ex: laptop lid switch). This program handles reading the event file to know when to execute macros. It’s usage is explained in the wiki page of the repo.",
    "crumbs": [
      "Keyboard",
      "ev-morse"
    ]
  },
  {
    "objectID": "evmorse.html#suppressing-keys",
    "href": "evmorse.html#suppressing-keys",
    "title": "Macros independent of the windowing system",
    "section": "suppressing keys",
    "text": "suppressing keys\nThe above, however, is not enough. The keys are still being sent to the windowing system. To deal with this it is necessary to remap the scancode in the hardware database (hwdb) to a keycode that is unused. Luckily, there is a list of keycodes to be found in (/usr/include/linux/input-event-codes.h). Conveniently, there are keys MACRO1-MACRO30 which are perfect for our use case. Armed with evtest and this arch wiki article, the results should be a file (/etc/udev/hwdb.d/50-filename.hwdb) that looks something like:\nevdev:atkbd:dmi:bvn*:bvr*:bd*:svn*:pn*:pvr*\n KEYBOARD_KEY_3b=macro1\n KEYBOARD_KEY_3c=macro2\n KEYBOARD_KEY_3d=macro3\n KEYBOARD_KEY_3e=macro4\n KEYBOARD_KEY_3f=macro5\n KEYBOARD_KEY_40=macro6",
    "crumbs": [
      "Keyboard",
      "ev-morse"
    ]
  },
  {
    "objectID": "evmorse.html#symlink",
    "href": "evmorse.html#symlink",
    "title": "Macros independent of the windowing system",
    "section": "symlink",
    "text": "symlink\nev-morse needs a device file to be ran, but what about devices that do not connect predictably on boot. A laptop keyboard would have a consistent event file, but the same can’t be said for headphones or other peripheral devices. For these devices it is necessary to create a symbolic link with a udev rule. First we need get some unique identifying information on the device with udevadm info -a /dev/input/~~~~. With this information we create a udev rule to establish the symbolic link. The result should be a file (/etc/udev/rules.d/50-filename.rules) that looks something like:\nSUBSYSTEMS==\"input\", ATTRS{name}==\"headphone name (AVRCP)\", ATTRS{id/product}==\"000a\", SYMLINK+=\"headphones\"\nSource for making udev symlink here. Additionally, multiple rules can be put in one file, separating by new lines.\nThe end.",
    "crumbs": [
      "Keyboard",
      "ev-morse"
    ]
  },
  {
    "objectID": "MLDL/imtools.html",
    "href": "MLDL/imtools.html",
    "title": "A collection of some code I find useful",
    "section": "",
    "text": "fp = Path(\"./Data\")\n\nfailed = verify_images(get_image_files(fp))\nfailed.map(fp.unlink)\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    get_y=parent_label,\n    splitter=RandomSplitter(valid_pct=0.25),\n    item_tfms=[Resize(192, method=\"squish\")]\n).dataloaders(fp)\n\nAfter processing a set of unsorted images with this ipynb. This block of code will load the output into a datablock in fastai, handling train/valid split and formatting the images with item_tfms.",
    "crumbs": [
      "ML/DL",
      "Image",
      "Tools"
    ]
  },
  {
    "objectID": "MLDL/imglabelernb.html",
    "href": "MLDL/imglabelernb.html",
    "title": "img-labeler-nb preview",
    "section": "",
    "text": "A simple interface for categorizing images into labeled folders in ipynb. Useful for quickly prepping categorical image data for deep learning / ML.\nBe sure to run each cell.\n1️⃣. First create a list of the Categories to separate into.\nIt may be helpful to include a “Trash” grouping for images that are outside of scope.\nThe key variable takes a string of keys. This sets key bindings for the keys to categories respectively.\n\ncategories = [\"Interior\", \"Exterior\", \"Trash\"]\nkey = \"123\"\n# key = \"asd\" \n\n2️⃣. Select the input and output folders\n\nimport ipywidgets as widgets\nimport os\ninput = widgets.Dropdown(options=[x for x in os.listdir() if \".\" not in x], # folders with \".\" are excluded\n                        description=\"Input folder:\",)\noutput = widgets.Dropdown(options=[x for x in os.listdir() if \".\" not in x], # folders with \".\" are excluded\n                        description=\"Out Folder:\",)\ndisplay(widgets.VBox([input, output]))\n\n\n3️⃣. Prepare output folders (creates folders for categories that are not present)\n\nfor fold in [x for x in categories if x not in [x for x in os.listdir(output.value) if \".\" not in x]]:\n    os.mkdir(output.value + \"/\" + fold)\n\n4️⃣. Sort\n\ndef nextpic():\n    global imgpos\n    imgpos+=1\n    imgframe.value = open(input.value + \"/\" + piclist[imgpos],\"rb\").read()\n\ndef send2folder(fold):\n    global foldlist\n    foldlist.append(fold)\n    os.rename(input.value + \"/\" + piclist[imgpos], output.value + \"/\" + fold + \"/\" + piclist[imgpos])\n    nextpic()\n\ndef txtev(s):\n    if len(s.new) &gt; len(s.old):\n        if x:= keydict.get(s.new[-1]):\n            send2folder(x)\n\ndef oopsgoback1(_a):\n    global imgpos, foldlist\n    imgpos-=1\n    os.rename(output.value + \"/\" + foldlist.pop() + \"/\" + piclist[imgpos], input.value + \"/\" + piclist[imgpos])\n    imgframe.value = open(input.value + \"/\" + piclist[imgpos],\"rb\").read()\n\npiclist = [x for x in os.listdir(input.value) if \".\" in x]\nfoldlist = []\nimgpos=0\nkeydict = {k: cat for k, cat in zip(list(key), categories)}\n\nimage0 = open(input.value + \"/\" + piclist[0],\"rb\").read()\nimgframe = widgets.Image(value=image0\n                        , width=300 \n                        # , height=300\n                        )\n\nbutlist = [widgets.Button(description=cat, button_style=\"danger\") for cat in categories]\n[button.on_click(lambda x, imgpos=cat: send2folder(imgpos)) for button, cat in zip(butlist, categories)]\n\ntxtkey = widgets.Text(placeholder=\"type here for key bindings\")\ntxtkey.observe(txtev, \"value\")\n\noopsbutton = widgets.Button(description=\"Take me back\", button_style=\"warning\")\noopsbutton.on_click(oopsgoback1)\n\ndisplay(widgets.HBox(butlist), txtkey, imgframe, oopsbutton)\n\n\n📂Example of loading in the now sorted data (fastai datablock)\n\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nfp = Path(\"./categories\")\nDataBlock(blocks=(ImageBlock, CategoryBlock), \n        get_items=get_image_files, \n        get_y=parent_label,\n        # splitter=RandomSplitter(valid_pct=0.25),\n        item_tfms=[Resize(256, method=\"squish\")]\n).dataloaders(fp).show_batch(max_n=9)"
  },
  {
    "objectID": "MLDL/imglabelernb.html#dataset-labeler-tool",
    "href": "MLDL/imglabelernb.html#dataset-labeler-tool",
    "title": "img-labeler-nb preview",
    "section": "",
    "text": "A simple interface for categorizing images into labeled folders in ipynb. Useful for quickly prepping categorical image data for deep learning / ML.\nBe sure to run each cell.\n1️⃣. First create a list of the Categories to separate into.\nIt may be helpful to include a “Trash” grouping for images that are outside of scope.\nThe key variable takes a string of keys. This sets key bindings for the keys to categories respectively.\n\ncategories = [\"Interior\", \"Exterior\", \"Trash\"]\nkey = \"123\"\n# key = \"asd\" \n\n2️⃣. Select the input and output folders\n\nimport ipywidgets as widgets\nimport os\ninput = widgets.Dropdown(options=[x for x in os.listdir() if \".\" not in x], # folders with \".\" are excluded\n                        description=\"Input folder:\",)\noutput = widgets.Dropdown(options=[x for x in os.listdir() if \".\" not in x], # folders with \".\" are excluded\n                        description=\"Out Folder:\",)\ndisplay(widgets.VBox([input, output]))\n\n\n3️⃣. Prepare output folders (creates folders for categories that are not present)\n\nfor fold in [x for x in categories if x not in [x for x in os.listdir(output.value) if \".\" not in x]]:\n    os.mkdir(output.value + \"/\" + fold)\n\n4️⃣. Sort\n\ndef nextpic():\n    global imgpos\n    imgpos+=1\n    imgframe.value = open(input.value + \"/\" + piclist[imgpos],\"rb\").read()\n\ndef send2folder(fold):\n    global foldlist\n    foldlist.append(fold)\n    os.rename(input.value + \"/\" + piclist[imgpos], output.value + \"/\" + fold + \"/\" + piclist[imgpos])\n    nextpic()\n\ndef txtev(s):\n    if len(s.new) &gt; len(s.old):\n        if x:= keydict.get(s.new[-1]):\n            send2folder(x)\n\ndef oopsgoback1(_a):\n    global imgpos, foldlist\n    imgpos-=1\n    os.rename(output.value + \"/\" + foldlist.pop() + \"/\" + piclist[imgpos], input.value + \"/\" + piclist[imgpos])\n    imgframe.value = open(input.value + \"/\" + piclist[imgpos],\"rb\").read()\n\npiclist = [x for x in os.listdir(input.value) if \".\" in x]\nfoldlist = []\nimgpos=0\nkeydict = {k: cat for k, cat in zip(list(key), categories)}\n\nimage0 = open(input.value + \"/\" + piclist[0],\"rb\").read()\nimgframe = widgets.Image(value=image0\n                        , width=300 \n                        # , height=300\n                        )\n\nbutlist = [widgets.Button(description=cat, button_style=\"danger\") for cat in categories]\n[button.on_click(lambda x, imgpos=cat: send2folder(imgpos)) for button, cat in zip(butlist, categories)]\n\ntxtkey = widgets.Text(placeholder=\"type here for key bindings\")\ntxtkey.observe(txtev, \"value\")\n\noopsbutton = widgets.Button(description=\"Take me back\", button_style=\"warning\")\noopsbutton.on_click(oopsgoback1)\n\ndisplay(widgets.HBox(butlist), txtkey, imgframe, oopsbutton)\n\n\n📂Example of loading in the now sorted data (fastai datablock)\n\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nfp = Path(\"./categories\")\nDataBlock(blocks=(ImageBlock, CategoryBlock), \n        get_items=get_image_files, \n        get_y=parent_label,\n        # splitter=RandomSplitter(valid_pct=0.25),\n        item_tfms=[Resize(256, method=\"squish\")]\n).dataloaders(fp).show_batch(max_n=9)"
  }
]